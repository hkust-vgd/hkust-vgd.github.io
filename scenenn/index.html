<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>SceneNN: A Scene Meshes Dataset with aNNotations</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/scenenn.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

  <!-- Google icon
  -------------------------------------------------- -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

  <!-- jQuery
  -------------------------------------------------- -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>

  <!-- Analytics
  -------------------------------------------------- -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-86869673-1', 'auto');
  ga('send', 'pageview');
  </script>

  <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
  <style>
    img {
        display: block;
    }
    figure {
        padding: 0;
        margin: 0;
        overflow: hidden;
        cursor: pointer;
    }
    .hover figure img {
        -webkit-transform: rotate(0) scale(1);
        transform: rotate(0) scale(1);

    }
    .hover figure:hover img {
        -webkit-transition: .3s ease-in-out;
        transition: .3s ease-in-out;
        -webkit-transform: rotate(15deg) scale(1.4);
        transform: rotate(15deg) scale(1.4);
    }
  </style>
</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">


        <h4><b>SceneNN</b>: A Scene Meshes Dataset with aNNotations</h4>

        <p style="margin-bottom:12px;"><a class="simple" href="http://sonhua.github.io">Binh-Son Hua</a><sup>1</sup>, <a class="simple" href="https://pqhieu.github.io/">Quang-Hieu Pham</a><sup>2</sup>, <a class="simple" href="http://ducthanhnguyen.weebly.com">Duc Thanh Nguyen</a><sup>3</sup>, Minh-Khoi Tran<sup>2</sup>, <a class="simple" href="https://www.cs.umb.edu/~craigyu/">Lap-Fai Yu</a><sup>4</sup>, and <a class="simple" href="http://www.saikit.org">Sai-Kit Yeung</a><sup>5</sup></p>

        <p style="margin-bottom:20px;">
		<sup>1</sup>The University of Tokyo
        <span style="display:inline-block; width: 32px"></span>
        <sup>2</sup>Singapore University of Technology and Design
        <span style="display:inline-block; width: 32px"></span>
        <sup>3</sup>Deakin University
		</br>
        <span style="display:inline-block; width: 32px"></span>
        <sup>4</sup>George Mason University
		<span style="display:inline-block; width: 32px"></span>
        <sup>5</sup>The Hong Kong University of Science and Technology
        </p>

        <!--<img src="images/overview.png" class="u-max-full-width"></img>-->
        <!--<iframe width="100%" height="500px" scrolling="no" style="border:0" src="overview.html"></iframe>-->
        <div id="teaser" class="container" style="width:100%; margin:0; padding:0">

        </div>

        <p>We introduce an RGB-D scene dataset consisting of more than 100 indoor scenes.
        Our scenes are captured at various places, e.g., offices, dormitory, classrooms, pantry, etc.,
        from University of Massachusetts Boston and Singapore University of Technology and Design.
        <br>
        All scenes are reconstructed into triangle meshes and have
        per-vertex and per-pixel annotation. We further enriched
        the dataset with fine-grained information such as
        axis-aligned bounding boxes, oriented bounding boxes, and object poses.
        </p>

        <!-- ------------------------------------- -->

        <div class="section">
            <h5>News</h5>

            <ul class="news">
            <li><b>Oct 28, 2019</b>: Our paper about a new object dataset named <a href="https://hkust-vgd.github.io/scanobjectnn/">ScanObjectNN</a> (derived from SceneNN and ScanNet) for real-world object classification is accepted to ICCV 2019 as an oral! See you in Seoul!
            <li><b>Feb 25, 2019</b>: Our paper <a href="https://pqhieu.github.io/research/cvpr19/">JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds</a> got accepted to CVPR 2019 as an oral!
            <li><b>Jan 08, 2019</b>: We presented the paper <a href="https://pqhieu.github.io/research/wacv19/">Real-time Progressive 3D Semantic Segmentation for Indoor Scenes</a> at <a href="http://wacv19.wacv.net/">WACV 2019</a> in Hawaii!
            <li><b>Oct 02, 2018</b>: We had fun running the tutorial at IROS 2018: <a href='http://iros18.scenenn.net'>Creating and Understanding 3D Annotated Scene Meshes</a>!
			<li><b>June 07, 2018</b>: Code and data of our paper <a href="http://cvpr18.scenenn.net">Pointwise Convolutional Neural Networks</a> (CVPR 2018) are released!
			<li><b>May 21, 2018</b>: We are at ICRA 2018 for the tutorial: <a href='http://icra18.scenenn.net'>Creating Annotated Scene Meshes for Training and Testing Robot Systems</a>.
			<li><b>Mar 18, 2018</b>: <a href="http://beta.magixhome.com/">MagixHome</a>: our synthetic 3D scene modeling platform is available now!
            <!--<li><b>Mar 10, 2018</b>: The track <a href="pdf/rgbd2cad_shrec18.pdf">report</a> for our SHREC'18: RGB-D-to-CAD Object Retrieval contest is available.-->
			<li><b>Jan 22, 2018</b>: We organize the <a href='http://shrec18.scenenn.net'>RGB-D-to-CAD Object Retrieval contest</a> at <a href="http://www.shrec.net/">Eurographics 2018 Workshop on 3D Object Retrieval</a>.
            <li><b>Oct 13, 2017</b>: Experimental WebGL annotation tool is <a href="http://webgl.scenenn.net">here</a>.</li>
            <li><b>Aug 04, 2017</b>: The SceneNN annotation tool paper is accepted to TVCG and presented at Pacific Graphics 2017. See <a href="http://tvcg17.scenenn.net">here</a>.</li>
            <li><b>Jan 16, 2017</b>: We organize a new SHREC 2017 track named <a href="http://shrec17.scenenn.net">RGB-D Object-to-CAD Retrieval</a>.
            <!--
            <li><b>Nov 25, 2016</b>: 50 scenes from SceneNN with semantic classes (as defined in NYU-D v2) are released in the <a href="https://drive.google.com/drive/folders/0B2BQi-ql8CzeUkJNbEVEQkozSmc">beta repository</a>.</li>
            -->
            <li><b>Oct 28, 2016</b>: SceneNN received <a href="images/HonorableMention2.jpg">Best Paper Honorable Mention</a> at 3DV 2016!
            <!--<li><b>Sep 17, 2016</b>: More than 90 scenes in the dataset are uploaded!</li>-->
            </ul>
        </div>

        <!-- ------------------------------------- -->

        <div class="section">
            <h5>Dataset & Tools</h5>

			<p><b><a href="http://cvpr18.scenenn.net">CVPR 2018</a></b> (evaluating semantic segmentation with NYU-D v2 <a href="cvpr18/data/nyu_color.xml">40 classes</a>)</p>
            <div class="container">
			  <div class="row">
                
                <div class="four columns">
                    <i class="material-icons grey-icon">cloud</i>
                    <br>
                    <a href="http://hkust-vgd.ust.hk/scenenn/home/cvpr18/data/scenenn_seg_76.zip">HKUST server</a>
                    <br>
                    (Training data, 8.7 GB)
                </div>                

                <div class="four columns">
                    <i class="material-icons grey-icon">cloud</i>
                    <br>
                    <a href="https://drive.google.com/file/d/1DcKKrdzirUjKejjasexGdU0ECMpIXCD3/view">Google Drive</a>
                    <br>
                    (Mirror link, 8.7 GB)
                </div>
                
                 <div class="four columns">
                    <i class="material-icons grey-icon">cloud</i>
                    <br>
                    <a href="https://drive.google.com/open?id=1d_ILfaxpJBpiiwCZtvC4jEKnixEr9N2l">Google Drive</a>
                    <br>
                    (Raw PLY files, 4.0 GB)
                </div>
                
			  </div>
			</div>

			<!---------------------------------------------------- -->
			<br>

			<p><b>3DV 2016</b> (original instance annotation of 100+ scenes)</p>
			<div class="container">
              <div class="row">
                
                <div class="two columns">
                    <i class="material-icons grey-icon">cloud</i>
                    <br>
                    <a href="http://hkust-vgd.ust.hk/scenenn/">HKUST server</a>
                    <br>

                </div>

                <!--
                <div class="two columns">
                    <i class="material-icons grey-icon">cloud</i>
                    <br>
                    <a href="https://drive.google.com/drive/folders/0B-aa7y5Ox4eZWE8yMkRkNkU4Tk0?usp=sharing">Google Drive</a>
                    <br>
                    (120 GB)

                </div>

                <div class="two columns">
                    <i class="material-icons grey-icon">cloud_download</i>
                    <br>
                    <a href="https://github.com/scenenn/scenenn">Download script</a>
                    <br>
                    (Python)
                </div>
                -->
                
                <div class="two columns">
                    <i class="material-icons grey-icon">create</i>
                    <br>
                    <a href="https://github.com/scenenn/sese">Annotation tool</a>
                    <br>
                    (Windows x64)
                </div>

                <div class="two columns">
                    <i class="material-icons grey-icon">photo</i>
                    <br>
                    <a href="https://github.com/scenenn/ElasticReconstruction">RGBD reconstruction</a>
                </div>
              </div>
            </div>

            <br>

            <ul>

            <li>
            Please contact us for the annotation tool on Linux platform.
            </li>

            </ul>
        </div>

        <!-- ------------------------------------- -->

        <div class="section publication">
            <h5>Publication</h5>

            <ul>

              <li>
                <div>
                <p class="paper title">JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds with Multi-Task Pointwise Networks and Multi-Value Conditional Random Fields</p>
                <p>Quang-Hieu Pham, Duc Thanh Nguyen, Binh-Son Hua, Gemma Roig, and Sai-Kit Yeung</p>
                <p class="paper venue">Computer Vision and Pattern Recognition CVPR 2019 (<font color="#BB4444"><b>Oral</b></font>)</p>
                <a href="https://pqhieu.github.io/research/cvpr19">Project</a>
                <a href="https://github.com/pqhieu/jsis3d">Code</a>
                </div>
            </li>

            <li>
                <div>
                <p class="paper title">Real-time Progressive 3D Semantic Segmentation for Indoor Scenes</p>
                <p>Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung</p>
                <p class="paper venue">WACV 2019</p>
                <a href="https://pqhieu.github.io/research/wacv19">Project</a>
                </div>
            </li>

            <li>
                <div>
                <p class="paper title">Pointwise Convolutional Neural Networks</p>
                <p>Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung</p>
                <p class="paper venue">Computer Vision and Pattern Recognition (CVPR) 2018</p>
                <a href="cvpr18/index.html">Project</a>
                <a href="https://arxiv.org/abs/1712.05245">Paper</a>
                <a href="https://github.com/scenenn/pointwise">Code</a>
                <a id="abibPointwise" href="#">Bibtex</a>
                </div>
<!-- Don't indent this for correct bibtex display -->
<pre id="bibPointwise">
<code>@inproceedings{hua-pointwise-cvpr18,
    title = {Pointwise Convolutional Neural Networks},
    author = {Binh-Son Hua and Minh-Khoi Tran and Sai-Kit Yeung},
    booktitle = {Computer Vision and Pattern Recognition (CVPR)},
    year = {2018}
}</code>
</pre>
<!-- -->
            </li>

            <li>
                <div>
                <p class="paper title">SceneNN: A Scene Meshes Dataset with aNNotations</p>
                <p>Binh-Son Hua, Quang-Hieu Pham, Duc Thanh Nguyen, Minh-Khoi Tran, Lap-Fai Yu, and Sai-Kit Yeung</p>
                <p class="paper venue">International Conference on 3D Vision (3DV) 2016. <a class="red" href="images/HonorableMention2.jpg">Best Paper Honorable Mention</a>.</p>
                <a href="pdf/dataset_3dv16.pdf">Paper</a>
                <a href="pdf/dataset_supp_3dv16.pdf">Supplemental</a>
                <a href="pdf/scenenn_slides_3dv16.pdf">Slides</a>
                <a href="pdf/scenenn_poster_3dv16.pdf">Poster</a>
                <a id="abibSceneNN" href="#">Bibtex</a>
                </div>
<!-- Don't indent this for correct bibtex display -->
<pre id="bibSceneNN">
<code>@inproceedings{scenenn-3dv16,
    author = {Binh-Son Hua and Quang-Hieu Pham and Duc Thanh Nguyen and Minh-Khoi Tran and Lap-Fai Yu and Sai-Kit Yeung},
    title = {SceneNN: A Scene Meshes Dataset with aNNotations},
    booktitle = {International Conference on 3D Vision (3DV)},
    year = {2016}
}</code>
</pre>
<!-- -->
            </li>

            <li>
                <div>
                <p class="paper title">A Robust 3D-2D Interactive Tool for Scene Segmentation and Annotation</p>
                <p>Duc Thanh Nguyen, Binh-Son Hua, Lap-Fai Yu, and Sai-Kit Yeung</p>
                <p class="paper venue">IEEE Transactions on Visualization and Computer Graphics (TVCG) 2017</p>
                <p class="paper venue"><i>(presented at Pacific Graphics 2017)</i></p>
                <a href="pdf/annotation_tvcg17.pdf">Paper</a>
                <a href="video/annotation.mp4">Video</a>
                <a id="abibTool" href="#">Bibtex</a>
                </div>
<!-- Don't indent this for correct bibtex display -->
<pre id="bibTool">
<code>@article{anno-tvcg17,
    author = {Thanh Nguyen, Duc and Hua, Binh-Son and Yu, Lap-Fai and Yeung, Sai-Kit},
    title = {A Robust 3D-2D Interactive Tool for Scene Segmentation and Annotation},
    journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
    year = {2017}
}
</code>
</pre>
<!-- -->
                <br>
                <div class="row">
                <div class="nine columns">
                <video id="my-video" class="u-max-full-width" preload="auto" poster="video/annotation.png">
                    <source src="video/annotation.mp4" type="video/mp4">
                </video>
                </div>
                </div>
            </li>
            </ul>
        </div>

        <!-- ------------------------------------- -->

        <div class="section publication">
            <h5>Technical Report</h5>

            <ul>

              <li>
                <div>
                  <p class="paper title">SHREC'18: RGB-D Object-to-CAD Retrieval</p>
                  <p>Organizers: Quang-Hieu Pham, Binh-Son Hua, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung</p>
                  <p class="paper venue">Eurographics Workshop on 3D Object Retrieval 2018</p>
                  <a href="pdf/rgbd2cad_shrec18.pdf">Paper</a>
                  <a href="http://shrec18.scenenn.net">Homepage</a>
                </div>
              </li>

            <li>
                <div>
                <p class="paper title">SHREC'17: RGB-D to CAD Retrieval with ObjectNN Dataset</p>
                <p>Organizers: Binh-Son Hua, Quang-Trung Truong, Minh-Khoi Tran, Quang-Hieu Pham, Lap-Fai Yu, Duc Thanh Nguyen, and Sai-Kit Yeung</p>
                <p class="paper venue">Eurographics Workshop on 3D Object Retrieval 2017</p>
                <a href="pdf/objectnn_shrec17.pdf">Paper</a>
                <a href="pdf/objectnn_shrec17_slides.pptx">Slides</a>
                <a href="http://shrec17.scenenn.net">Homepage</a>
                <a id="abibShrec17" href="#">Bibtex</a>
                </div>
<!-- Don't indent this for correct bibtex display -->
<pre id="bibShrec17">
<code>@misc{objectnn-shrec17,
    author = {Binh-Son Hua and Quang-Trung Truong and Minh-Khoi Tran and Quang-Hieu Pham and
              Asako Kanezaki and Tang Lee and HungYueh Chiang and Winston Hsu and
              Bo Li and Yijuan Lu and Henry Johan and Shoki Tashiro and Masaki Aono and
              Minh-Triet Tran and Viet-Khoi Pham and Hai-Dang Nguyen and Vinh-Tiep Nguyen and
              Quang-Thang Tran and Thuyen V. Phan and Bao Truong and Minh N. Do and Anh-Duc Duong and
              Lap-Fai Yu and Duc Thanh Nguyen and Sai-Kit Yeung},
    title = {SHREC'17: RGB-D to CAD Retrieval with ObjectNN Dataset},
    booktitle = {Eurographics Workshop on 3D Object Retrieval},
    year = {2017}
}</code>
</pre>
<!-- -->
            </li>
            </ul>
        </div>

        <!-- ------------------------------------- -->

        <div class="section">
            <h5>Discussion</h5>
            <p>
            Please email us at <b><u>scenenn [at] gmail.com</u></b> for any inquiries.
            You can also post to the discussion board below.
            </p>
        </div>

        <!-- Disqus
        ------------------------------------------ -->
        <div id="disqus_thread"></div>
        <script>
        var disqus_config = function () {
        this.page.url = "http://www.scenenn.net";  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = "scenenn"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };
        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'http://scenenn.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
        </script>
        <noscript>
        Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
        <br>

        <!-- ------------------------------------- -->

        <div class="section">
            <h5>Call for Contributions</h5>

            <p>If you find this dataset useful and would like to contribute, please do not hesitate to let us know. Below are some potential ideas that you can help with to expand the dataset:
            <ul>
                <li>Capture new scenes and contribute raw RGB-D videos.</li>
                <li>Refine annotation of existing scenes.</li>
                <li>Annotate scenes from other datasets (<a href="http://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU Depth v2</a>, <a href="http://sun3d.cs.princeton.edu/">SUN3D</a>) with our annotation tool.
                <li>Reconstruct the difficult scenes into higher quality.</li>
            </ul>
            </p>
        </div>

        <!-- ------------------------------------- -->

        <div class="section">
            <h5>Acknowledgements</h5>
            <p>
            We are grateful to the anonymous reviewers for their constructive comments.
            We thank Fangyu Lin for his assistance with the data capture and development of the WebGL viewer, and Guoxuan Zhang for his help with the early version of the annotation tool.
            </p>

            <p>
            Lap-Fai Yu is supported by the University of Massachusetts Boston StartUp Grant P20150000029280 and by the Joseph P. Healey Research Grant Program provided by the Office of the Vice Provost for Research and Strategic Initiatives & Dean of Graduate Studies of the University of Massachusetts Boston. This research is supported by the National Science Foundation under award number 1565978. We also acknowledge NVIDIA Corporation for graphics card donation.
            </p>

            <p>
            Sai-Kit Yeung is supported by Singapore MOE Academic Research Fund MOE2013-T2-1-159 and SUTD-MIT International Design Center Grant IDG31300106. We acknowledge the support of the SUTD Digital Manufacturing and Design (DManD) Centre which is supported by the National Research Foundation (NRF) of Singapore. This research is also supported by the National Research Foundation, Prime Minister's Office, Singapore under its IDM Futures Funding Initiative.
            </p>
        </div>
  </div>

  <script>
    function show_scene_list() {
        // Populate teaser images (a 3x4 grid of images)
        var height = 3;
        var width = 4;

        // Change the scene ID here for other scenes
        var images = ["076",        "086",      "032",      "082",
                      "049",        "093",      "246",      "021",
                      "207",        "213",      "272",      "074"];

        $("#teaser").empty();
        for (var i = 0; i < height; ++i) {
            var row = "<div class=\"row\" style=\"padding-bottom: 1%;\">";
            for (var j = 0; j < width; ++j) {
                var id = images[i * width + j];
                var file = "images/scenes/" + id + "/" + id + "_segmented.png";
                row += "<div class=\"three columns hover\" style='margin-left: 1%;'>";
                // Change margin above to adjust gaps between columns
                row += "<figure><img id=\"" + id + "\" class=\"u-max-full-width\" src=\"" + file + "\" data-popup-open=\"popup-1\"></img></figure>";
                row += "</div>";
            }
            row += "</div>"

            $("#teaser").append(row);
        }

        // Add click event for each image
        /*
        for (var i = 0; i < height; ++i) {
            for (var j = 0; j < width; ++j) {
                var id = images[i * width + j];
                $("#"+id).click(function(e) {
                    //window.location.href = "load_ply.html?ply_name=data/066_merge_color_lowres.ply&xml_name=data/066_merge_correct.xml";

                    // test scene
                    show_webgl("data/066_merge_color_lowres.ply",
                                "data/066_merge_correct.xml");
                });
            }
        }*/
    }

    $(document).ready(function(){
        var bibName = ["SceneNN", "Tool", "Pointwise", "Shrec17"];

        // Add click event for bibtex links
        for (var i = 0; i < bibName.length; ++i) {
            $("#abib" + bibName[i]).click(function(e){

                var abib = event.target.id; // <a> that fires the event
                var bib = abib.replace("abib", "bib");

                $("#"+bib).toggle(100);

                // Scroll to the code position
                $('html, body').animate({
                    scrollTop: $("#"+bib).offset().top
                }, 100);

                // Cancel the default action (jump to page top due to link clicked)
                e.preventDefault();
            });

            // Hide at start
            $("#bib" + bibName[i]).hide();
        }

        show_scene_list();

        // When video is hovered, show the controls
        $('#my-video').hover(function() {
            if (this.hasAttribute("controls")) {
                this.removeAttribute("controls")
            } else {
                this.setAttribute("controls", "controls")
            }
        });
    });
  </script>

  <!-- Disqus comment count
  -------------------------------------------------- -->
  <script id="dsq-count-scr" src="http://scenenn.disqus.com/count.js" async></script>

  <!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
